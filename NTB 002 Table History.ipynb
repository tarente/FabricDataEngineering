{"cells":[{"cell_type":"code","source":["%run \"NTB 000 Utils\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c617d93d-2948-4417-a628-67ecdd16d814"},{"cell_type":"code","source":["# Get the delta table history for strTableName\n","def fnGetDeltaTableHistory (strTableName, strColumnName, strGroupByColumnName=\"\", boolversionAsOf=True, intNrVersions=100, intIndentationLevel = 0):\n","    fltStartTime   = time.time()\n","    strNumSpaces   = fnGetIndentationString(intIndentationLevel)\n","    strCurrentDate = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    print(f\"{strCurrentDate}: {strNumSpaces}Executing - fnGetDeltaTableHistory('{strTableName}', '{strColumnName}', '{strGroupByColumnName}', '{boolversionAsOf}', '{intNrVersions}', '{intIndentationLevel}').\")\n","\n","    # Set strAsOfProperty\n","    strAsOfProperty = \"versionAsOf\"\n","    if (~ boolversionAsOf):\n","        strAsOfProperty = \"timestampAsOf\"\n","\n","    # Set strNewGroupByColumnName\n","    strNewGroupByColumnName = strGroupByColumnName\n","    # If empty then force group by existing column 'tableVersion'\n","    if (strGroupByColumnName == \"\"):\n","        strNewGroupByColumnName = \"tableVersion\"\n","\n","    # Create empty dataframe\n","    dfTableHist = (\n","        spark\n","            .table(strTableName)\n","            .where(lit(False) == lit(True)) # Force empty row\n","            .withColumn(\"tableVersion\", lit(0).cast(\"bigint\"))\n","            .withColumn(\"tableTimestamp\", lit(0).cast(\"timestamp\"))\n","            .groupBy(\"tableVersion\", \"tableTimestamp\", strNewGroupByColumnName)\n","            .agg(min(strColumnName).alias(f\"min{strColumnName}\"),\n","                 max(strColumnName).alias(f\"max{strColumnName}\"),\n","                 count(lit(1)).alias(\"NrRows\")\n","            )\n","            .orderBy(strNewGroupByColumnName)\n","    )\n","\n","    # Get all history for strTableName\n","    strDescribeHistory = f\"describe history {strTableName}\"\n","\n","    # Create iterator for the history versions limited by intNrVersions\n","    iteTableHist = (\n","         spark\n","            .sql(strDescribeHistory)\n","            .limit(intNrVersions)\n","            .withColumn(\"tableVersion\",   col(\"version\"))\n","            .withColumn(\"tableTimestamp\", col(\"timestamp\"))\n","            .select(\"tableVersion\",\n","                    \"tableTimestamp\"\n","            )\n","            .rdd\n","            .toLocalIterator()\n","    )\n","\n","    # Iterate for all selected history versions\n","    try:\n","        for rowVersion in iteTableHist:\n","            strVersion   = rowVersion[0]\n","            strTimestamp = rowVersion[1]\n","\n","            # Set strAsOfPropertyValue\n","            strAsOfPropertyValue = strVersion\n","            if (strAsOfProperty == \"timestampAsOf\"):\n","                strAsOfPropertyValue = strTimestamp\n","\n","            dfTable = (\n","                spark\n","                    .read\n","                    .option(strAsOfProperty, strAsOfPropertyValue)\n","                    .table(strTableName)\n","                    .withColumn(\"tableVersion\", lit(strVersion).cast(\"bigint\"))\n","                    .withColumn(\"tableTimestamp\", lit(strTimestamp).cast(\"timestamp\"))\n","                    .groupBy(\"tableVersion\", \"tableTimestamp\", strNewGroupByColumnName)\n","                    .agg(min(strColumnName).alias(f\"min{strColumnName}\"),\n","                         max(strColumnName).alias(f\"max{strColumnName}\"),\n","                         count(lit(1)).alias(\"NrRows\")\n","                    )\n","                    .orderBy(strNewGroupByColumnName)\n","                    .cache()\n","            )\n","\n","            dfTableHist = (\n","                dfTableHist\n","                    .union(dfTable\n","                )\n","                .cache()\n","            )\n","\n","    except Exception as e:\n","        print(f\"{e}\")\n","\n","    strCurrentDate = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    strDuration    = fnGetDurationAsString(fltStartTime)\n","    print(f\"{strCurrentDate}: {strNumSpaces}Finished  - fnGetDeltaTableHistory('{strTableName}', '{strColumnName}', '{strGroupByColumnName}', '{boolversionAsOf}', '{intNrVersions}', '{intIndentationLevel}') in '{strDuration}'.\")\n","\n","    return dfTableHist\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ca93a055-93c6-4e4b-a454-105a1e8474b7"},{"cell_type":"code","source":["# Get the delta table history for strTableName\n","# TBC - adding 'where clause' to be applied to the each table version to restrict the number of rows returned\n","def fnGetDeltaTableHistoryDynamic (strTableName, strColumnName, strListGroupByColumnNames=\"\", boolversionAsOf=True, intNrVersions=100, intIndentationLevel = 0):\n","    fltStartTime   = time.time()\n","    strNumSpaces   = fnGetIndentationString(intIndentationLevel)\n","    strCurrentDate = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    print(f\"{strCurrentDate}: {strNumSpaces}Executing - fnGetDeltaTableHistoryDynamic('{strTableName}', '{strColumnName}', '{strListGroupByColumnNames}', '{boolversionAsOf}, '{intNrVersions}', '{intIndentationLevel}').\")\n","\n","    # Set strAsOfProperty\n","    strAsOfProperty = \"versionAsOf\"\n","    if (~ boolversionAsOf):\n","        strAsOfProperty = \"timestampAsOf\"\n","\n","    # Get all history for strTableName\n","    strDescribeHistory = f\"describe history {strTableName}\"\n","\n","    # Set variables\n","    strVersion   = \"0\"\n","    strTimestamp = strCurrentDate\n","\n","    # Needs to be defined before strSelect!\n","    # Set strNewListGroupByColumnNames\n","    strNewListGroupByColumnNames = strListGroupByColumnNames\n","    strGroupBy                   = \"\"\n","\n","    # If not empty then add a ',' to the end\n","    if (strGroupByColumnName != \"\"):\n","        strNewListGroupByColumnNames = f\"\"\"\n","       {strListGroupByColumnNames},\"\"\"\n","\n","        strGroupBy = f\"\"\"\n"," group by {strListGroupByColumnNames}\n"," order by {strListGroupByColumnNames}\"\"\"\n","\n","    strSelect = f\"\"\"\n","select {strVersion}         as tableVersion,\n","       '{strTimestamp}'     as tableTimestamp,{strNewListGroupByColumnNames}\n","       min({strColumnName}) as min{strColumnName},\n","       max({strColumnName}) as max{strColumnName},\n","       count(*)             as NrRows\n","  from {strTableName}\"\"\"\n","\n","    strWhere = f\"\"\"\n"," where 0 = 1\"\"\"\n","\n","    strAsOf = f\"\"\" version as of {strVersion}\"\"\"\n","    if (~ boolversionAsOf):\n","        strAsOf = f\"\"\" timestamp as of '{strTimestamp}'\"\"\"\n","\n","    strQuery = f\"\"\"\n","{strSelect}/*{strAsOf}*/{strWhere}{strGroupBy}\n","\"\"\"\n","\n","    # Create empty dataframe\n","    dfTableHist = (\n","        spark\n","            .sql(strQuery)\n","            .where(lit(0) == lit(1))\n","            .cache()\n","    )\n","\n","    # Get all history for strTableName\n","    strDescribeHistory = f\"describe history {strTableName}\"\n","\n","    # Create iterator for the history versions limited by intNrVersions\n","    iteTableHist = (\n","         spark\n","            .sql(strDescribeHistory)\n","            .limit(intNrVersions)\n","            .withColumn(\"tableVersion\",   col(\"version\"))\n","            .withColumn(\"tableTimestamp\", col(\"timestamp\"))\n","            .select(\"tableVersion\",\n","                    \"tableTimestamp\"\n","            )\n","            .rdd\n","            .toLocalIterator()\n","    )\n","\n","    # Iterate for all selected history versions\n","    try:\n","        for rowVersion in iteTableHist:\n","            strVersion   = rowVersion[0]\n","            strTimestamp = rowVersion[1]\n","\n","            strSelect = f\"\"\"\n","select {strVersion}         as tableVersion,\n","       '{strTimestamp}'     as tableTimestamp,{strNewListGroupByColumnNames}\n","       min({strColumnName}) as min{strColumnName},\n","       max({strColumnName}) as max{strColumnName},\n","       count(*)             as NrRows\n","  from {strTableName}\"\"\"\n","\n","            strWhere = f\"\"\"\n"," where 1 = 1\"\"\"\n","\n","            strAsOf = f\"\"\" version as of {strVersion}\"\"\"\n","            if (~ boolversionAsOf):\n","                strAsOf = f\"\"\" timestamp as of '{strTimestamp}'\"\"\"\n","\n","            strQuery = f\"\"\"\n","{strSelect}{strAsOf}{strWhere}{strGroupBy}\n","\"\"\"\n","\n","            dfTable = (\n","                spark\n","                    .sql(strQuery)\n","                    .cache()\n","            )\n","\n","            dfTableHist = (\n","                dfTableHist\n","                    .union(dfTable\n","                )\n","                .cache()\n","            )\n","\n","    except Exception as e:\n","        print(f\"{e}\")\n","\n","    strCurrentDate = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    strDuration    = fnGetDurationAsString(fltStartTime)\n","    print(f\"{strCurrentDate}: {strNumSpaces}Finished  - fnGetDeltaTableHistoryDynamic('{strTableName}', '{strColumnName}', '{strListGroupByColumnNames}', '{boolversionAsOf}, '{intNrVersions}', '{intIndentationLevel}') in '{strDuration}'.\")\n","\n","    return dfTableHist\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"55924e5e-1047-4b95-91f2-4d921e8e0625"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}